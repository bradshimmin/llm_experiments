{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script together is installed in '/opt/python/3.10.8/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Basic setup \n",
    "# Thanks to https://www.youtube.com/@samwitteveenai\n",
    "\n",
    "!pip -q install langchain huggingface_hub tiktoken\n",
    "!pip -q install --upgrade together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.270\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/python/3.10.8/lib/python3.10/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# Verify Langchain version\n",
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environmental variables, if necessary\n",
    "import os\n",
    "\n",
    "# List of avail vars\n",
    "# os.environ['OPENAI_API_KEY']\n",
    "# os.environ['HUGGINGFACEHUB_API_TOKEN'] \n",
    "# os.environ['GOOGLE_API_KEY'] \n",
    "# os.environ['TOGETHER_API_KEY'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done\n"
     ]
    }
   ],
   "source": [
    "# Connect to Together.ai for hosted LLM\n",
    "import together\n",
    "\n",
    "# set API key\n",
    "together.api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 EleutherAI/gpt-j-6b\n",
      "1 EleutherAI/pythia-12b-v0\n",
      "2 EleutherAI/pythia-1b-v0\n",
      "3 EleutherAI/pythia-2.8b-v0\n",
      "4 EleutherAI/pythia-6.9b\n",
      "5 HuggingFaceH4/starchat-alpha\n",
      "6 NousResearch/Nous-Hermes-13b\n",
      "7 NousResearch/Nous-Hermes-Llama2-13b\n",
      "8 NumbersStation/nsql-6B\n",
      "9 OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "10 OpenAssistant/stablelm-7b-sft-v7-epoch-3\n",
      "11 bigcode/starcoder\n",
      "12 databricks/dolly-v2-12b\n",
      "13 databricks/dolly-v2-3b\n",
      "14 databricks/dolly-v2-7b\n",
      "15 google/flan-t5-xl\n",
      "16 huggyllama/llama-13b\n",
      "17 huggyllama/llama-30b\n",
      "18 huggyllama/llama-65b\n",
      "19 huggyllama/llama-7b\n",
      "20 lmsys/fastchat-t5-3b-v1.0\n",
      "21 lmsys/vicuna-13b-v1.3\n",
      "22 lmsys/vicuna-7b-v1.3\n",
      "23 prompthero/openjourney\n",
      "24 runwayml/stable-diffusion-v1-5\n",
      "25 stabilityai/stable-diffusion-2-1\n",
      "26 stabilityai/stable-diffusion-xl-base-1.0\n",
      "27 stabilityai/stablelm-base-alpha-3b\n",
      "28 stabilityai/stablelm-base-alpha-7b\n",
      "29 tatsu-lab/alpaca-7b-wdiff\n",
      "30 timdettmers/guanaco-7b\n",
      "31 togethercomputer/GPT-JT-6B-v1\n",
      "32 togethercomputer/GPT-JT-Moderation-6B\n",
      "33 togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
      "34 togethercomputer/Koala-7B\n",
      "35 togethercomputer/LLaMA-2-7B-32K\n",
      "36 togethercomputer/Llama-2-7B-32K-Instruct\n",
      "37 togethercomputer/Pythia-Chat-Base-7B-v0.16\n",
      "38 togethercomputer/RedPajama-INCITE-7B-Base\n",
      "39 togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "40 togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "41 togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "42 togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "43 togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "44 togethercomputer/codegen2-7B\n",
      "45 togethercomputer/falcon-7b-instruct\n",
      "46 togethercomputer/falcon-7b\n",
      "47 togethercomputer/llama-2-13b-chat\n",
      "48 togethercomputer/llama-2-13b\n",
      "49 togethercomputer/llama-2-70b-chat\n",
      "50 togethercomputer/llama-2-70b\n",
      "51 togethercomputer/llama-2-7b-chat\n",
      "52 togethercomputer/llama-2-7b\n",
      "53 togethercomputer/mpt-30b-chat\n",
      "54 togethercomputer/mpt-7b-chat\n",
      "55 togethercomputer/mpt-7b-instruct\n",
      "56 togethercomputer/mpt-7b\n",
      "57 togethercomputer/replit-code-v1-3b\n"
     ]
    }
   ],
   "source": [
    "# Build a list available models from Together.ai\n",
    "models = together.Models.list()\n",
    "\n",
    "# Print full list of available models\n",
    "for idx, model in enumerate(models):\n",
    "    print(idx, model['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "togethercomputer/llama-2-7b-chat\n"
     ]
    }
   ],
   "source": [
    "# Double check the model we want to use\n",
    "# togethercomputer/llama-2-7b-chat\n",
    "print(models[51]['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'value': '54e8c0ada9901e58698eaa8fbe85a38a5f47c38f6ad228c4a21d38e9fb4e86eb'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start up our model\n",
    "together.Models.start(\"togethercomputer/llama-2-7b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import logging\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "from pydantic import Extra, Field\n",
    "\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "# Create a container class for our LLM\n",
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models setup\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-7b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    # class Config:\n",
    "    #     extra = Extra.forbid\n",
    "\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set.\"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
    "        )\n",
    "        values[\"together_api_key\"] = api_key\n",
    "        return values\n",
    "\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        together.api_key = self.together_api_key\n",
    "        output = together.Complete.create(prompt,\n",
    "                                          model=self.model,\n",
    "                                          max_tokens=self.max_tokens,\n",
    "                                          temperature=self.temperature,\n",
    "                                          )\n",
    "        text = output['output']['choices'][0]['text']\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup sytem message\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return assistant_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model with basic params\n",
    "\n",
    "my_model = \"togethercomputer/llama-2-7b-chat\"\n",
    "\n",
    "llm = TogetherLLM(\n",
    "    model= my_model,\n",
    "    temperature=0.1,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic language translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are an advanced assistant that excels at translation. \n",
      "<</SYS>>\n",
      "\n",
      "Convert the following text from English to Spanish. Do not repeate the initial text to be translated; simply return the translation:\n",
      "\n",
      " {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "# Setup system prompt template\n",
    "system_prompt = \"You are an advanced assistant that excels at translation. \"\n",
    "instruction = \"Convert the following text from English to Spanish. Do not repeate the initial text to be translated; simply return the translation:\\n\\n {text}\"\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain for prompt and model\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Where's the bathroom?\n",
      "Spanish:  ¡Dónde está el baño?\n"
     ]
    }
   ],
   "source": [
    "# Let's run a translation request\n",
    "text = \"Where's the bathroom?\"\n",
    "output = llm_chain.run(text)\n",
    "print(f\"English: {text}\\nSpanish: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close down Together model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "together.Models.stop(\"togethercomputer/llama-2-7b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
